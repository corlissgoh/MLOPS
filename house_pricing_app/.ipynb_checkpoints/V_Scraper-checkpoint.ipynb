{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Web Scraper for animeultima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get URLs of different pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " # URLs for different anime genres\n",
    "genres = [\"Action\", \"Adventure\", \"Comedy\", \"Romance\", \"Slice of Life\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Action': 'https://animeultima.su/genres',\n",
       " 'Adventure': 'https://animeultima.su/genres',\n",
       " 'Comedy': 'https://animeultima.su/genres',\n",
       " 'Romance': 'https://animeultima.su/genres',\n",
       " 'Slice of Life': 'https://animeultima.su/genres'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base link\n",
    "link = \"https://animeultima.su/genres/{}\"\n",
    "\n",
    "\n",
    "# dict to store urls\n",
    "url_dict = {}\n",
    "\n",
    "# making a GET request\n",
    "for genre in genres: \n",
    "    formated_url = url.format(genre)\n",
    "    url_dict[genre] = formated_url\n",
    "\n",
    "url_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing show info by sending request to specified url and returning a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This response is then conerted to an HTML form using Beautiful Soup and Ixml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specified url to send request to\n",
    "url = \"https://animeultima.su/genres/action\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sending a  request to the specified url\n",
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert response to Beautiful Soup Object\n",
    "content = bs(resp.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Iterating through list of animes\n",
    "# for anime in content.select('.tt'):\n",
    "    \n",
    "#     try:\n",
    "# #         create a python dict\n",
    "#         data = {\n",
    "#             \"title\":anime.select('.tt')[0].get_text().strip(),\n",
    "# #             \"type\":anime.select('.typez TV')[0].get_text().strip()\n",
    "#         }\n",
    "#     except IndexError:\n",
    "#         continue\n",
    "        \n",
    "#     print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Alice Gear Aegis ExpansionAlice Gear Aegis Expansion'}\n",
      "{'title': 'Ao no Exorcist: Shimane Illuminati-henAo no Exorcist: Shimane Illuminati-hen'}\n",
      "{'title': 'Arifureta Shokugyou de Sekai Saikyou: Maboroshi no Bouken to Kiseki no KaigouArifureta Shokugyou de Sekai Saikyou: Maboroshi no Bouken to Kiseki no Kaigou'}\n",
      "{'title': 'Arknights: Fuyukomori KaerimichiArknights: Fuyukomori Kaerimichi'}\n",
      "{'title': 'Arknights: Reimei ZensouArknights: Reimei Zensou'}\n",
      "{'title': 'Ars no KyojuuArs no Kyojuu'}\n",
      "{'title': 'Ayakashi TriangleAyakashi Triangle'}\n",
      "{'title': 'Baraou no SouretsuBaraou no Souretsu'}\n",
      "{'title': 'Berserk: Ougon Jidai-hen – Memorial EditionBerserk: Ougon Jidai-hen – Memorial Edition'}\n",
      "{'title': 'Black Clover: Mahou Tei no KenBlack Clover: Mahou Tei no Ken'}\n"
     ]
    }
   ],
   "source": [
    "# chatgpt ver\n",
    "# Itertaing through a list of animes\n",
    "# titles are in an ‘div’ element,, inside the '.tt' class\n",
    "for anime in content.select('.tt'):\n",
    "#   catches any exceptions that might occur during the extraction process\n",
    "    try:\n",
    "#        get text content of anime tag, stripping leading or trailing whitespaces \n",
    "        title = anime.get_text(strip=True)\n",
    "#       store dat in dictionary\n",
    "        data = {\n",
    "            \"title\": title\n",
    "        }\n",
    "        print(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing anime: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crteate scraping function, an be reused several times for different URLs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anime(url):\n",
    "    \n",
    "    # sending a  request to the specified url\n",
    "    resp = requests.get(url)\n",
    "    # convert response to Beautiful Soup Object\n",
    "    content = bs(resp.content, 'lxml')\n",
    "    \n",
    "    # dict to store urls\n",
    "    anime_list = []\n",
    "#   Itertaing through a list of movies\n",
    "#   titles are in an ‘div’ element,, inside the '.tt' class\n",
    "    for anime in content.select('.tt'):\n",
    "        \n",
    "        try:\n",
    "#         create a python dict to contain all info parsed from webpage and return pandas dataframe\n",
    "            data = {\n",
    "            \"title\":anime.select('.tt')[0].get_text().strip(),\n",
    "#             \"type\":anime.select('.typez TV')[0].get_text().strip()\n",
    "        }\n",
    "        except IndexError:\n",
    "            continue\n",
    "        \n",
    "#       add date to dict  \n",
    "        anime_list.append(data)\n",
    "        \n",
    "    return pd.DataFrame(anime_list)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  function creates a python dictionary that contains all the information we parsed from\n",
    "# the web page and then it return a pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# url being scraped\n",
    "url = \"https://animeultima.su/genres/action\"\n",
    "\n",
    "# Calling function\n",
    "get_anime(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scarping animes of different genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  dataframe to store in\n",
    "df_data = pd.DataFrame()\n",
    "\n",
    "\n",
    "# Iterating through the dictionary of genres\n",
    "for genre, url in url_dict.items():\n",
    "#   Calling the get_anime function to scrape anime data for the current genr\n",
    "    df_data = pd.concat([df_data, get_anime(url)])\n",
    "    \n",
    "# Writing the combined DataFrame to a CSV file\n",
    "df_data.to_csv('anime.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************Spare code***********************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get links for all animes under each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # URLs for different anime genres\n",
    "# tags = [\"action\", \"adventure\", \"comedy\", \"romance\", \"slice-of-life\"]\n",
    "# genres = [\"Action\", \"Adventure\", \"Comedy\", \"Romance\", \"Slice of Life\"]\n",
    "\n",
    "# # Base link\n",
    "# link = \"https://www.crunchyroll.com/genre/{}\"\n",
    "\n",
    "# # Making a GET request\n",
    "# urls = {}\n",
    "# for tag in tags:\n",
    "#     formated_url = link.format(tag) \n",
    "#     urls[genres[tags.index(tag)]] = formated_url\n",
    "\n",
    "# urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_url = \"https://www.crunchyroll.com\"\n",
    "# anime_links = {}\n",
    "\n",
    "# # Function to scrape pages (1-10) of each genre\n",
    "# def scrape_page(url,num):\n",
    "#     url = url + \"?page=\" + str(num)\n",
    "#     response = requests.get(url)\n",
    "#     html = response.content\n",
    "#     soup = bs(html, \"html.parser\")\n",
    "\n",
    "#     # Getting all the links for the anime\n",
    "#     hrefs = soup.find_all('a', class_=\"dynamic-name\")\n",
    "#     for href in hrefs:\n",
    "#         link = base_url + href.get('href')\n",
    "#         anime_links[genre].append(link)\n",
    "\n",
    "# animes =  {}\n",
    "# for genre in urls.keys():\n",
    "#     anime_links[genre] = []\n",
    "#     num = 1\n",
    "#     while num <= 10: \n",
    "#     # As there are a lot of animes per genre, we will only scrape the first 10 pages of each genre\n",
    "#         url = urls[genre]\n",
    "#         scrape_page(url,num)\n",
    "#         if not soup.find(class_=[\"disabled\"]):\n",
    "#             print(f\"{genre}: {len(anime_links[genre])}\")\n",
    "#             break\n",
    "#         num += 1\n",
    "#     anime = pd.DataFrame(anime_links[genre], columns=['link'])\n",
    "#     anime['genre'] = genre\n",
    "#     animes[genre] = anime\n",
    "\n",
    "# animes.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing\n",
    "# base_site = \"https://www.crunchyroll.com/\"\n",
    "\n",
    "# response = requests.get(base_site)\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html = response.content\n",
    "# html[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # making the soup\n",
    "# soup = bs(html,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Export html to file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Wiki_response.html', 'wb') as file:\n",
    "#     file.write(soup.prettify('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Wiki_response.html', 'wb') as file:\n",
    "#     file.write(soup.prettify('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animes = pd.concat(animes.values(), ignore_index=True)\n",
    "# animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "# animes = animes.drop_duplicates(subset=['link'])\n",
    "# animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animes['genre'].value_counts().plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Anime Title and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titles = []\n",
    "# descriptions = []\n",
    "# for link in animes['link']:\n",
    "#     response = requests.get(link)\n",
    "#     html = response.content\n",
    "#     soup = bs(html, \"html.parser\")\n",
    "\n",
    "#     # Getting the title of the anime\n",
    "#     title = soup.select(\"h2.film-name\")[0].get_text().strip()\n",
    "#     titles.append(title)\n",
    "    \n",
    "\n",
    "#     # Getting the description\n",
    "#     description = soup.select(\"p.shorting\")[0].get_text().strip()\n",
    "#     descriptions.append(description)\n",
    "\n",
    "# animes['title'] = titles\n",
    "# animes['description'] = descriptions\n",
    "# animes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to CSV (Export html to file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animes.to_csv('animes.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
