{
 "cells": [
  {
   "attachments": {
    "NYPLogo.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NYPLogo.png](attachment:NYPLogo.png)\n",
    "\n",
    "# IT2311 Assignment - Task 2: Sentiment Classification\n",
    "\n",
    "You are required to build a sentiment classification model predict the sentiment of the review text. Businesses will be able to use this model to predict the sentiment of a new review.\n",
    "\n",
    "Complete the following sub-tasks:\n",
    "1. **Load Data**: Load the clean dataset\n",
    "2. **Data Preparation**: Prepares the text representation for this task\n",
    "3. **Modelling**: Perform sentiment classification using different text representation and modelling techniques\n",
    "4. **Evaluation**: Evaluates results from the algorithms and select the best model\n",
    "\n",
    "For each sub-task, perform the necessary steps and **explain the rationale taken for each step in this Jupyter notebook**. \n",
    "\n",
    "**Done by: \\<Enter your name and admin number here\\>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and download the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Classification \n",
    "\n",
    "# analyzing the sentiment of a given text or document and categorizing the\n",
    "# text/document  into a specific class or category (like positive and negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifies any particular text or document as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification is done for two classes: positive and negative. However,\n",
    "# we can add more classes like neutral, highly positive, highly negative, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text_length</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Positive</td>\n",
       "      <td>393</td>\n",
       "      <td>tried drunken noodle spicy chicken wing crispy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Negative</td>\n",
       "      <td>2184</td>\n",
       "      <td>unfortunately site doe permit deservingly appr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Positive</td>\n",
       "      <td>654</td>\n",
       "      <td>looking new car husband told bozarth chevy dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Positive</td>\n",
       "      <td>92</td>\n",
       "      <td>guy fast friendly knowledgeable oil change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Positive</td>\n",
       "      <td>869</td>\n",
       "      <td>writing review based wife positive interaction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39875</th>\n",
       "      <td>2</td>\n",
       "      <td>Negative</td>\n",
       "      <td>2756</td>\n",
       "      <td>advise dirt bike current condition pro rental ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39876</th>\n",
       "      <td>1</td>\n",
       "      <td>Negative</td>\n",
       "      <td>777</td>\n",
       "      <td>disappointed ive discount tire customer year g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39877</th>\n",
       "      <td>5</td>\n",
       "      <td>Positive</td>\n",
       "      <td>473</td>\n",
       "      <td>great price great product great staff professi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39878</th>\n",
       "      <td>5</td>\n",
       "      <td>Positive</td>\n",
       "      <td>600</td>\n",
       "      <td>bought son truck noticed noise tire took lana ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39879</th>\n",
       "      <td>5</td>\n",
       "      <td>Positive</td>\n",
       "      <td>164</td>\n",
       "      <td>went lopressure signal kia rio fixed tire scre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39880 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       stars sentiment  text_length  \\\n",
       "0          4  Positive          393   \n",
       "1          1  Negative         2184   \n",
       "2          5  Positive          654   \n",
       "3          5  Positive           92   \n",
       "4          5  Positive          869   \n",
       "...      ...       ...          ...   \n",
       "39875      2  Negative         2756   \n",
       "39876      1  Negative          777   \n",
       "39877      5  Positive          473   \n",
       "39878      5  Positive          600   \n",
       "39879      5  Positive          164   \n",
       "\n",
       "                                              clean_text  \n",
       "0      tried drunken noodle spicy chicken wing crispy...  \n",
       "1      unfortunately site doe permit deservingly appr...  \n",
       "2      looking new car husband told bozarth chevy dec...  \n",
       "3             guy fast friendly knowledgeable oil change  \n",
       "4      writing review based wife positive interaction...  \n",
       "...                                                  ...  \n",
       "39875  advise dirt bike current condition pro rental ...  \n",
       "39876  disappointed ive discount tire customer year g...  \n",
       "39877  great price great product great staff professi...  \n",
       "39878  bought son truck noticed noise tire took lana ...  \n",
       "39879  went lopressure signal kia rio fixed tire scre...  \n",
       "\n",
       "[39880 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('221225F_final.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Perform the necessary steps and explain the rationale taken here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fileid \u001b[38;5;129;01min\u001b[39;00m movie_reviews\u001b[38;5;241m.\u001b[39mfileids():\n\u001b[0;32m      6\u001b[0m     sentiment, filename \u001b[38;5;241m=\u001b[39m fileid\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     reviews\u001b[38;5;241m.\u001b[39mappend((filename, movie_reviews\u001b[38;5;241m.\u001b[39mraw(fileid), \u001b[43mtext\u001b[49m))\n\u001b[0;32m      9\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(reviews, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstars\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the text review dataset\n",
    "from nltk.corpus import movie_reviews \n",
    "\n",
    "reviews = []\n",
    "for fileid in movie_reviews.fileids():\n",
    "    sentiment, filename = fileid.split('/')\n",
    "    reviews.append((filename, movie_reviews.raw(fileid), text))\n",
    "\n",
    "df = pd.DataFrame(reviews, columns=['', 'text', 'stars'])\n",
    "print(df.shape)\n",
    "display(df.head())\n",
    "\n",
    "# Plotting the Sentiment distribution\n",
    "plt.figure()\n",
    "pd.value_counts(df['sentiment']).plot.bar(title=\"Sentiment Distribution\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"No. of rows in df\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24268\\3992679031.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobability\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mall_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# print first 10 words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mall_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6200\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6201\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6202\u001b[0m         ):\n\u001b[0;32m   6203\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6204\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "all_words = [word.lower() for sent in df.text for word in word_tokenize(sent)]\n",
    "    \n",
    "# print first 10 words\n",
    "print (all_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Frequency Distribution of all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    " \n",
    "all_words_frequency = FreqDist(all_words)\n",
    "print (all_words_frequency)\n",
    "\n",
    "# print 10 most frequently occurring words\n",
    "print (all_words_frequency.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Punctuation and Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m                 all_words_clean\u001b[38;5;241m.\u001b[39mappend(porter_stemmer\u001b[38;5;241m.\u001b[39mstem(punc_free))\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_words_clean\n\u001b[1;32m---> 18\u001b[0m all_words_clean \u001b[38;5;241m=\u001b[39m clean(\u001b[43mall_words\u001b[49m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# print the first 10 words\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m (all_words_clean[:\u001b[38;5;241m10\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_words' is not defined"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "stopwords_english = stopwords.words('english')\n",
    "\n",
    "def clean(doc):\n",
    "    all_words_clean = []\n",
    "    for word in doc:\n",
    "        if word not in stopwords_english and not word.isdigit():\n",
    "            punc_free = ''.join([ch for ch in word if ch not in string.punctuation])\n",
    "            if len(punc_free)>2 and not word.isdigit():\n",
    "                all_words_clean.append(porter_stemmer.stem(punc_free))\n",
    "\n",
    "    return all_words_clean\n",
    "\n",
    "all_words_clean = clean(all_words)\n",
    "\n",
    "# print the first 10 words\n",
    "print (all_words_clean[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Distribution of cleaned words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_words_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m all_words_frequency \u001b[38;5;241m=\u001b[39m FreqDist(\u001b[43mall_words_clean\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m (all_words_frequency)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# print 10 most frequently occurring words\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_words_clean' is not defined"
     ]
    }
   ],
   "source": [
    "all_words_frequency = FreqDist(all_words_clean)\n",
    "print (all_words_frequency)\n",
    "\n",
    "# print 10 most frequently occurring words\n",
    "print (all_words_frequency.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Word Feature using 2000 most frequently occurring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_words_frequency' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[43mall_words_frequency\u001b[49m)) \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# get 2000 frequently occuring words\u001b[39;00m\n\u001b[0;32m      4\u001b[0m most_common_words \u001b[38;5;241m=\u001b[39m all_words_frequency\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_words_frequency' is not defined"
     ]
    }
   ],
   "source": [
    "print (len(all_words_frequency)) \n",
    " \n",
    "# get 2000 frequently occuring words\n",
    "most_common_words = all_words_frequency.most_common(2000)\n",
    "\n",
    "# print the first 10 most frequently occuring words\n",
    "print (most_common_words[:10])\n",
    "\n",
    "# print the last 10 most frequently occuring words\n",
    "print (most_common_words[1990:])\n",
    "\n",
    "# the most common words list's elements are in the form of tuple get \n",
    "# only the first element of each tuple of the word list\n",
    "word_features = [item[0] for item in most_common_words]\n",
    "print (word_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: word_tokenize(x\u001b[38;5;241m.\u001b[39mlower()))\n\u001b[0;32m      2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: clean(x))  \n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(lambda x: word_tokenize(x.lower()))\n",
    "df['text'] = df['text'].apply(lambda x: clean(x))  \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that will be used to create feature set. \n",
    "# The feature set is # used to train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m         doc_features\u001b[38;5;241m.\u001b[39mappend(features)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc_features\n\u001b[1;32m---> 11\u001b[0m feature_set \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mdocument_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, index \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m     12\u001b[0m feature_set\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m, in \u001b[0;36mdocument_features\u001b[1;34m(df, stemmed_tokens)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m      4\u001b[0m     features \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mword_features\u001b[49m:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;66;03m# get term occurence: true if it's in the word_features, false if it's not\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         features[word] \u001b[38;5;241m=\u001b[39m (word \u001b[38;5;129;01min\u001b[39;00m row[stemmed_tokens])\n\u001b[0;32m      8\u001b[0m     doc_features\u001b[38;5;241m.\u001b[39mappend(features)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_features' is not defined"
     ]
    }
   ],
   "source": [
    "def document_features(df, stemmed_tokens):\n",
    "    doc_features = []\n",
    "    for index, row in df.iterrows():\n",
    "        features = {}\n",
    "        for word in word_features:\n",
    "            # get term occurence: true if it's in the word_features, false if it's not\n",
    "            features[word] = (word in row[stemmed_tokens])\n",
    "        doc_features.append(features)\n",
    "    return doc_features\n",
    "\n",
    "feature_set = pd.DataFrame(document_features(df, 'text'), index = df.index)\n",
    "feature_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "Perform the necessary steps and explain the rationale taken here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Train and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 4\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_set\u001b[49m\n\u001b[0;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m df[df\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]]\n\u001b[0;32m      7\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_set' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = feature_set\n",
    "y = df[df.columns[-1:]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print (y_train.sentiment.value_counts(normalize=True))\n",
    "\n",
    "#plot chart\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=y_train, x='sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\user\\anaconda3\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\user\\anaconda3\\lib\\site-packages (from seaborn) (1.26.1)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from seaborn) (2.1.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from seaborn) (3.8.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (4.44.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (23.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.3->seaborn) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [15 lines of output]\n",
      "  The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  rather than 'sklearn' for pip commands.\n",
      "  \n",
      "  Here is how to fix this error in the main use cases:\n",
      "  - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "    (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  - if the 'sklearn' package is used by one of your dependencies,\n",
      "    it would be great if you take some time to track which package uses\n",
      "    'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  - as a last resort, set the environment variable\n",
      "    SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \n",
      "  More information is available at\n",
      "  https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feature_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 6\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_set\u001b[49m\n\u001b[0;32m      7\u001b[0m y \u001b[38;5;241m=\u001b[39m df[df\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]]\n\u001b[0;32m      9\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_set' is not defined"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n",
    "!pip install sklearn\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = feature_set\n",
    "y = df[df.columns[-1:]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print (y_train.sentiment.value_counts(normalize=True))\n",
    "\n",
    "#plot chart\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=y_train, x='sentiment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\u001b[0;32m      6\u001b[0m classifier \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m classifier\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# classification report\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "classifier = DecisionTreeClassifier(random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, classifier.predict(X_test)))\n",
    "\n",
    "# accuracy score\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(\"Accuracy Score: \" + str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     sns\u001b[38;5;241m.\u001b[39mheatmap(con_mat, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, annot_kws\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m16\u001b[39m}, fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mg\u001b[39m\u001b[38;5;124m'\u001b[39m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlues\u001b[39m\u001b[38;5;124m'\u001b[39m, cbar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#Ploting the confusion matrix\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m conf_matrix(\u001b[43my_test\u001b[49m, y_pred)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Print the confusion matrix to look at the prediction for test data\n",
    "# Function to create a confusion matrix \n",
    "def conf_matrix(y_test, pred_test):    \n",
    "    \n",
    "    # Creating a confusion matrix\n",
    "    con_mat = confusion_matrix(y_test, pred_test)\n",
    "    con_mat = pd.DataFrame(con_mat, range(2), range(2))\n",
    "   \n",
    "    #Ploting the confusion matrix\n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.set(font_scale=1.5) \n",
    "    sns.heatmap(con_mat, annot=True, annot_kws={\"size\": 16}, fmt='g', cmap='Blues', cbar=False)\n",
    "    \n",
    "#Ploting the confusion matrix\n",
    "conf_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_review\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_review\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: word_tokenize(x\u001b[38;5;241m.\u001b[39mlower()))\n\u001b[0;32m      9\u001b[0m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_review\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_review\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: clean(x))\n\u001b[1;32m---> 11\u001b[0m test_features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mdocument_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustom_review\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, index \u001b[38;5;241m=\u001b[39m df_test\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m (classifier\u001b[38;5;241m.\u001b[39mpredict(test_features))\n",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m, in \u001b[0;36mdocument_features\u001b[1;34m(df, stemmed_tokens)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m      4\u001b[0m     features \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mword_features\u001b[49m:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;66;03m# get term occurence: true if it's in the word_features, false if it's not\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         features[word] \u001b[38;5;241m=\u001b[39m (word \u001b[38;5;129;01min\u001b[39;00m row[stemmed_tokens])\n\u001b[0;32m      8\u001b[0m     doc_features\u001b[38;5;241m.\u001b[39mappend(features)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_features' is not defined"
     ]
    }
   ],
   "source": [
    "# output of the classifier by providing some custom reviews.\n",
    "# Negative review correctly classified as negative \n",
    "# Positive review is classified as negative\n",
    "data = {'custom_review': ['I hated the film. It was a disaster. Poor direction, bad acting.', \n",
    "                          'It was a wonderful and amazing movie. I loved it. Best direction, good acting.']}\n",
    "\n",
    "df_test = pd.DataFrame (data, columns = ['custom_review'])\n",
    "df_test['custom_review'] = df_test['custom_review'].apply(lambda x: word_tokenize(x.lower()))\n",
    "df_test['custom_review'] = df_test['custom_review'].apply(lambda x: clean(x))\n",
    "\n",
    "test_features = pd.DataFrame(document_features(df_test, 'custom_review'), index = df_test.index)\n",
    "print (classifier.predict(test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words using TF-IDF Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m corpora\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Build the dictionary\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m mydict \u001b[38;5;241m=\u001b[39m corpora\u001b[38;5;241m.\u001b[39mDictionary(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      6\u001b[0m vocab_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(mydict)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_bow_features\u001b[39m(df, stemmed_tokens):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Build the dictionary\n",
    "mydict = corpora.Dictionary(df['text'])\n",
    "vocab_len = len(mydict)\n",
    "\n",
    "def get_bow_features(df, stemmed_tokens):\n",
    "    test_features = []\n",
    "    for index, row in df.iterrows():\n",
    "        # Converting the tokens into the format that the model requires\n",
    "        features = gensim.matutils.corpus2csc([mydict.doc2bow(row[stemmed_tokens])],num_terms=vocab_len).toarray()[:,0]\n",
    "        test_features.append(features)\n",
    "    return test_features\n",
    "\n",
    "header = \",\".join(str(mydict[ele]) for ele in range(vocab_len))\n",
    "\n",
    "bow_features = pd.DataFrame(get_bow_features(df, 'text'),                            \n",
    "                            columns=header.split(','), index = df.index)\n",
    "bow_features.head()\n",
    "\n",
    "# For this practical, we will create the term weights using TF-IDF.\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "# Build the dictionary\n",
    "mydict = corpora.Dictionary(df['text'])\n",
    "vocab_len = len(mydict)\n",
    "corpus = [mydict.doc2bow(line) for line in df['text']]\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "\n",
    "def get_tfidf_features(df, stemmed_tokens):\n",
    "    test_features_tfidf = []\n",
    "    for index, row in df.iterrows():\n",
    "        doc = mydict.doc2bow(row[stemmed_tokens])\n",
    "        # Converting the tokens into the formet that the model requires\n",
    "        features = gensim.matutils.corpus2csc([tfidf_model[doc]], num_terms=vocab_len).toarray()[:,0]\n",
    "        test_features_tfidf.append(features)\n",
    "    return test_features_tfidf\n",
    "\n",
    "header = \",\".join(str(mydict[ele]) for ele in range(vocab_len))\n",
    "\n",
    "tfidf_features = pd.DataFrame(get_tfidf_features(df, 'text'),                            \n",
    "                            columns=header.split(','), index = df.index)\n",
    "tfidf_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Classifier and Calculating Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_features\u001b[49m\n\u001b[0;32m      3\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaive_bayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GaussianNB\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_features' is not defined"
     ]
    }
   ],
   "source": [
    "X = tfidf_features\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = DecisionTreeClassifier(random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, classifier.predict(X_test)))\n",
    "\n",
    "# accuracy score\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(\"Accuracy Score: \" + str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Enter code here\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#Ploting the confusion matrix\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m conf_matrix(\u001b[43my_test\u001b[49m, y_pred)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "#Enter code here\n",
    "\n",
    "#Ploting the confusion matrix\n",
    "conf_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Classifier with Custom Review¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_tfidf_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_review\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_review\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: word_tokenize(x\u001b[38;5;241m.\u001b[39mlower()))\n\u001b[0;32m      6\u001b[0m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_review\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_review\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: clean(x))\n\u001b[1;32m----> 8\u001b[0m test_features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mget_tfidf_features\u001b[49m(df_test, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_review\u001b[39m\u001b[38;5;124m'\u001b[39m),                            \n\u001b[0;32m      9\u001b[0m                             columns\u001b[38;5;241m=\u001b[39mheader\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m), index \u001b[38;5;241m=\u001b[39m df_test\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m (classifier\u001b[38;5;241m.\u001b[39mpredict(test_features))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_tfidf_features' is not defined"
     ]
    }
   ],
   "source": [
    "data = {'custom_review': ['I hated the film. It was a disaster. Poor direction, bad acting.', \n",
    "                          'It was a wonderful and amazing movie. I loved it. Best direction, good acting.']}\n",
    "\n",
    "df_test = pd.DataFrame (data, columns = ['custom_review'])\n",
    "df_test['custom_review'] = df_test['custom_review'].apply(lambda x: word_tokenize(x.lower()))\n",
    "df_test['custom_review'] = df_test['custom_review'].apply(lambda x: clean(x))\n",
    "\n",
    "test_features = pd.DataFrame(get_tfidf_features(df_test, 'custom_review'),                            \n",
    "                            columns=header.split(','), index = df_test.index)\n",
    "print (classifier.predict(test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This DecisionTreeClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Find out the most important features from the classification model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m importances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_importances_\u001b[49m)\n\u001b[0;32m      3\u001b[0m feature_importances \u001b[38;5;241m=\u001b[39m [(feature, \u001b[38;5;28mround\u001b[39m(importance, \u001b[38;5;241m10\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m feature, importance \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tfidf_features\u001b[38;5;241m.\u001b[39mcolumns, importances)]\n\u001b[0;32m      4\u001b[0m feature_importances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(feature_importances, key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:659\u001b[0m, in \u001b[0;36mBaseDecisionTree.feature_importances_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_importances_\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    643\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the feature importances.\u001b[39;00m\n\u001b[0;32m    644\u001b[0m \n\u001b[0;32m    645\u001b[0m \u001b[38;5;124;03m    The importance of a feature is computed as the (normalized) total\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;124;03m        (Gini importance).\u001b[39;00m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 659\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    661\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mcompute_feature_importances()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1461\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[1;32m-> 1461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This DecisionTreeClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# Find out the most important features from the classification model\n",
    "importances = list(classifier.feature_importances_)\n",
    "feature_importances = [(feature, round(importance, 10)) for feature, importance in zip(tfidf_features.columns, importances)]\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "top_i = 0\n",
    "for pair in feature_importances:\n",
    "    print('Variable: {:10} Importance: {}'.format(*pair))\n",
    "    if top_i == 20:\n",
    "        break\n",
    "    top_i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Perform the necessary steps and explain the rationale taken here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Export your completed work as HTML. Select **File** > **Download as** > **HTML (.html)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
