{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41d69432",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This practice covers the steps on how to scrape jobs from Indeed. Using python, bs4, selenium, and pandas, we'll be able to extract information from indeed.com and construct a pandas data frame. Before we begin, let's understand web scraping simply. \n",
    "\n",
    "Imagine if you are trying to get much information about something from various web pages and articles that need to be stored in a suitable format, for instance, an excel file. One way is to go through all those websites and write the useful information to the excel sheets manually. But programmers tend to do it in an easy way which is web scraping. Web scraping is the technique of extracting a large amount of data from different web pages that can be stored in a suitable format.\n",
    "\n",
    "## Scraping job details from Indeed\n",
    "Indeed is one of the largest American job listing portals which consists of millions of job listings all over the world from different small scale and large scale companies including startups. Scraping job details from indeed really helps you to get a large amount of information about different jobs, locations, actively hiring companies, ratings, etc.\n",
    "\n",
    "Here are the steps involved:\n",
    "\n",
    "1. Install and import necessary modules\n",
    "2. Send some basic queries like like job title or company name and location to the Indeed website using selenium\n",
    "3. Fetch the current URL after sending the queries to the website using selenium\n",
    "4. Parse the page using requests and Beautiful Soup\n",
    "5. Fetch the information about job title, company name, rating, location, simple description, date of posting, etc\n",
    "6. Store this information into a CSV file using pandas\n",
    "\n",
    "\n",
    "## Load Libraries\n",
    "First of all, we need to install some specific modules including a chrome driver for selenium. After installing the chrome driver move it to the working directory.\n",
    "\n",
    "We need to import the libraries that will be used for this practical. Here requests help to send an HTTP request using python, Selenium is an automation tool that helps here to send queries to the website, lxml can convert the page into XML or HTML format. bs4 module for parsing the web page and pandas to convert the data into a CSV file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb12b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7b75fee",
   "metadata": {},
   "source": [
    "## Sending job title and location using selenium\n",
    "Now let's create a function that sends queries to the web page and returns the current URL. This function opens indeed.com using the specified URL as one of its parameters. Then it sends the job title or company name and location to the site using selenium. After that, we'll get a new page and its URL which consists of all the job details related to the job title and location you have specified as its parameters. Lastly, it returns the current URL which consists of jobs and their details so that we can simply scrape it using Beautiful Soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0922d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_url(url, job_title,location):\n",
    "    # this is tested\n",
    "    service = Service(executable_path=)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813607e2",
   "metadata": {},
   "source": [
    "## Scraping jobs using Beautiful Soup\n",
    "Now let's get into the scraping part. We can use BeautifulSoup to scape data that we require like what we have covered in the previous lab. Try the following code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e9445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9346983e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cff0e7b",
   "metadata": {},
   "source": [
    "The driver.get() function returns the data of the entire webpage. The next step is to find the CSS selectors and retrieve the raw text inside the tags that contain these CSS selectors. The CSS selectors given in the code are probably the same on the web page but sometimes it may change.\n",
    "\n",
    "By looping through all the job posts we'll get much information about it. Lastly, we converted the data into a pandas data frame and simply returned it. You'll get the details about the job title, company name, rating, location, date of posting, and a simple job description. You can save it as a CSV file using df.to_csv(\"jobs.csv\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708e23b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e61d46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
