{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Libraries to help with machine learning\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data \n",
    "df = pd.read_csv(\"victor_clean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['genre'].value_counts().plot.pie(autopct='%1.1f%%')\n",
    "r,c = df.shape\n",
    "print(\"Number of rows: \",r)\n",
    "print(\"Number of columns: \",c)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the null values\n",
    "df = df.dropna()\n",
    "\n",
    "# Check null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df['description']\n",
    "Y = df['genre']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "print('The size of the original dataset is', X.shape)\n",
    "print('The size of the training dataset is', X_train.shape)\n",
    "print('The size of the test dataset is', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100), # Lan\n",
    "    \"Support Vector Machine\": SVC(kernel='linear'), # Gabriel\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(), # Gabriel\n",
    "    \"Logistic Regression\": LogisticRegression(), # Victor\n",
    "    \"Naive Bayes\": MultinomialNB(), # Lan\n",
    "    \"Decision Tree\": DecisionTreeClassifier() # Victor\n",
    "}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy of {model_name}: {accuracy}\")\n",
    "    print(f\"Classification Report of {model_name}:\\n{report}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above, the best model for predicting the genres of the animes is the Naive Bayes model. \n",
    "\n",
    "However for all the models, the accuracy rate is moderately low ranging from 48%-52%. This may be because off the following reasons\n",
    "\n",
    "1. Small dataset size: If there are only ~1000 samples total to train and evaluate on, that may not be enough data for the models to learn the patterns and relationships within each class. More training data could help improve accuracy.\n",
    "\n",
    "2. Imbalanced classes: From the support counts, it looks like the classes are imbalanced, with Comedy, Romance and Adventure having fewer samples than Action and Slice of Life. This makes it harder for models to properly learn the minority classes. Resampling or oversampling techniques could help address class imbalance.\n",
    "\n",
    "3. Noisy labels: There may be some inaccuracies or noise in the assigned labels of the training data that makes it hard for models to capture the correct signals. Cleaning up the labeling could help. Standard scale the data, overcome the data\n",
    "\n",
    "4. Features need engineering: The current features being used may not be fully representative of the patterns needed to distinguish classes. Creating, transforming and selecting more predictive features could potentially improve separability.\n",
    "\n",
    "5. Overfitting on train data: Some complex models like Random Forest may be overfitting too closely to the training data, hurting generalizability. More regularization, cross-validation and tuning could help reduce overfitting.\n",
    "\n",
    "6. Class ambiguity: The classes may inherently have significant overlap or ambiguities that make them hard to precisely separate. For example, some Romance anime may also be Slice of Life. Disambiguating classes more cleanly could help. libraries such as smote to remove rows of similar data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest hyperparameters\n",
    "rf_hyperparameters = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [None, 5, 10, 15],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Support Vector Machine hyperparameters\n",
    "svm_hyperparameters = {\n",
    "    \"C\": [0.1, 1, 10],\n",
    "    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"degree\": [3, 4, 5],\n",
    "    \"gamma\": [\"scale\", \"auto\"]\n",
    "}\n",
    "\n",
    "# Gradient Boosting hyperparameters\n",
    "gb_hyperparameters = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"learning_rate\": [0.01, 0.1, 1.0],\n",
    "    \"max_depth\": [3, 4, 5]\n",
    "}   \n",
    "\n",
    "# Logistic Regression hyperparameters\n",
    "lr_hyperparameters = {\n",
    "    \"penalty\": [\"l1\", \"l2\", \"elasticnet\", \"none\"],\n",
    "    \"C\": [0.1, 1, 10],\n",
    "    \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]\n",
    "}\n",
    "\n",
    "# Naive Bayes hyperparameters\n",
    "nb_hyperparameters = {\n",
    "    \"alpha\": [0.1, 0.5, 1.0],\n",
    "    \"fit_prior\": [True, False]\n",
    "}\n",
    "\n",
    "# Decision Tree hyperparameters\n",
    "dt_hyperparameters = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"splitter\": [\"best\", \"random\"],\n",
    "    \"max_depth\": [None, 5, 10, 15],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune model\n",
    "hyperparameters = {\n",
    "    \"Random Forest\": rf_hyperparameters,\n",
    "    \"Support Vector Machine\": svm_hyperparameters,\n",
    "    \"Gradient Boosting\": gb_hyperparameters,\n",
    "    \"Logistic Regression\": lr_hyperparameters,\n",
    "    \"Naive Bayes\": nb_hyperparameters,\n",
    "    \"Decision Tree\": dt_hyperparameters\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Tuning {model_name}...\")\n",
    "    grid = GridSearchCV(model, hyperparameters[model_name], cv=5)\n",
    "    grid.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    print(f\"Best hyperparameters for {model_name}:\")\n",
    "    print(grid.best_params_)\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Update model with best hyperparameters\n",
    "    models[model_name] = grid.best_estimator_\n",
    "\n",
    "# Evaluate model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy of {model_name}: {accuracy}\")\n",
    "    print(f\"Classification Report of {model_name}:\\n{report}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
